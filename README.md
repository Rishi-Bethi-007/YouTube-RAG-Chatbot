# ğŸ“º YouTube Multi-Video / Playlist RAG  
**Production-style Retrieval-Augmented Chatbot with Memory**

Chat with multiple YouTube videos (or entire playlists) using a scalable RAG architecture built with:

- **Streamlit** (ChatGPT-style UI)
- **Pinecone** (Vector database)
- **PostgreSQL** (Source of truth)
- **Redis** (Caching layer)
- **OpenAI** (Embeddings + LLM)
- **Reranking + Dedup + Evaluation (RAGAS)**

---

## ğŸš€ What This Project Solves

YouTube videos are long and difficult to search precisely.

This system allows you to:

- Ingest **multiple videos or playlists**
- Ask natural language questions
- Get **timestamped citations**
- Have **multi-turn conversations with memory**
- Evaluate retrieval quality
- Log latency per stage
- Deploy publicly

---

# ğŸ—ï¸ Architecture

User â†’ Streamlit Chat UI
â†“
Query Rewrite (memory-aware)
â†“
Embedding (cached)
â†“
Pinecone (vector search)
â†“
Postgres (fetch chunk text)
â†“
Rerank (LLM-based)
â†“
Answer Generation (chat-style + citations)


### Storage Design

| Component     | Responsibility |
|--------------|---------------|
| Pinecone     | Stores vectors + minimal metadata |
| PostgreSQL   | Stores full chunk text + video metadata |
| Redis        | Caches rewrite / embeddings / retrieval / chunk text |
| Streamlit    | UI + session memory |

---

# âœ¨ Features

## âœ… Multi-Video / Playlist Ingestion
- Automatic chunking
- Timestamp-aware segments
- Idempotent ingestion (dedup safe)
- Video-level ingestion tracking

## âœ… Production Caching (Redis)
- Query rewrite cache
- Embedding cache
- Retrieval cache
- Chunk text cache

## âœ… Reranking
LLM-based reranker improves precision by selecting the best context from top-K retrieved candidates.

## âœ… Conversation Memory
- Maintains rolling summary
- Supports follow-ups like:
  - â€œWhat about that part?â€
  - â€œExplain more about that conceptâ€
- Token-efficient summarization

## âœ… Deduplication
- Chunk-level SHA1 IDs
- Unique DB constraints
- Ingestion logs prevent re-embedding

## âœ… Evaluation Harness
- RAGAS metrics:
  - Faithfulness
  - Answer relevancy
- Stage-level latency tracking

## âœ… Observability
- Per-stage timing:
  - Rewrite
  - Embedding
  - Retrieval
  - DB fetch
  - Rerank
  - Generation
- Cache hit tracking

---

# ğŸ“‚ Project Structure



youtube-rag/
â”œâ”€ app.py
â”œâ”€ docker-compose.yml
â”œâ”€ requirements.txt
â”œâ”€ .env.example
â”œâ”€ data/
â”‚ â”œâ”€ transcripts/
â”‚ â””â”€ eval/testset.json
â””â”€ src/
â”œâ”€ config.py
â”œâ”€ cache.py
â”œâ”€ db.py
â”œâ”€ models.py
â”œâ”€ ingest.py
â”œâ”€ retrieve.py
â”œâ”€ rewrite.py
â”œâ”€ rerank.py
â”œâ”€ memory.py
â””â”€ eval/run_eval.py


---

# ğŸ§  How Memory Works

We maintain:

- Full chat in session
- A rolling **summary**
- Last N turns passed to:
  - Query rewriting
  - Answer generation

This enables contextual multi-turn dialogue without exploding token usage.

---

# ğŸ› ï¸ Local Setup

## 1ï¸âƒ£ Start Postgres + Redis

```bash
docker compose up -d

2ï¸âƒ£ Install dependencies
pip install -r requirements.txt

3ï¸âƒ£ Create .env

Copy .env.example and fill in:

OPENAI_API_KEY=
PINECONE_API_KEY=
DATABASE_URL=postgresql+psycopg2://rag:rag@localhost:5432/ragdb
REDIS_URL=redis://localhost:6379/0

4ï¸âƒ£ Run the app
streamlit run app.py

ğŸ§ª Run Evaluation
python -m src.eval.run_eval


Outputs:

Latency summary

RAGAS metrics

ğŸŒ Deployment (Free)
Recommended Stack

Streamlit Community Cloud

Neon (free Postgres)

Upstash (free Redis)

Pinecone free tier

Steps

Push repo to GitHub

Create Neon Postgres â†’ copy DATABASE_URL

Create Upstash Redis â†’ copy REDIS_URL

Deploy app on Streamlit Cloud

Add secrets in app settings

ğŸ“Š Production Design Decisions
Why Postgres?

Vector DB is not source of truth.
Text storage belongs in relational DB for:

Re-indexing

Analytics

Versioning

Data governance

Why Redis?

LLM apps are latency-sensitive.
Redis reduces:

Embedding cost

Pinecone round trips

DB pressure

Why Reranking?

Vector similarity alone is noisy.
Reranking dramatically improves answer precision.

Why Summary Memory?

Full chat history grows tokens exponentially.
Summaries maintain context efficiently.

ğŸ” Example Questions

â€œWhat are the main themes discussed?â€

â€œExplain the concept mentioned at the beginning.â€

â€œWhat does the speaker say about scalability?â€

â€œWhat about the part where he talks about embeddings?â€

â€œSummarize the difference between FAISS and Pinecone.â€

ğŸ“ˆ Future Improvements

MMR before reranking

Hybrid search (BM25 + vector)

Persistent chat sessions

User authentication

Cost logging per query

Structured JSON answers
